{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n",
      "Pandas version: 0.25.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"third_code/recommenders-master/\")\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import papermill as pm\n",
    "import time\n",
    "import os \n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '64'\n",
    "\n",
    "from reco_utils.common.timer import Timer\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_stratified_split\n",
    "from reco_utils.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from reco_utils.recommender.sar import SAR\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_entity_row(data):\n",
    "    \"\"\"删除完全一致的行\"\"\"\n",
    "    t = (2020, 4, 10, 0, 0, 0, 0, 0, 0)\n",
    "    time_end = time.mktime(t)\n",
    "\n",
    "    data['time_diff'] = data['qtime'].diff() * time_end\n",
    "    first_rec = data.time_diff.isnull() | (data.time_diff < 0)\n",
    "    data.loc[first_rec, 'time_diff'] = -1\n",
    "    data['timestamp'] = data['qtime'] * time_end\n",
    "    return data[data['time_diff'] != 0]\n",
    "\n",
    "    \n",
    "def gen_data(dir_name, predict=False, drop_dup=True):    \n",
    "    all_files = [dir_name+f for f in os.listdir(dir_name) if \"csv\" in f and 'click' in f]\n",
    "    li = []\n",
    "    for filename in all_files:\n",
    "        print(filename)\n",
    "        df = pd.read_csv(filename, header=None, names=['user_id', 'item_id', 'qtime'])\n",
    "        li.append(df)\n",
    "\n",
    "    df_full = pd.concat(li, axis=0, ignore_index=True)\n",
    "    df_full.sort_values(by=['user_id', 'qtime'], inplace=True)\n",
    "    print(\"before drop length:\", len(df_full))\n",
    "    if drop_dup:\n",
    "        df_full = filter_entity_row(df_full)\n",
    "        print(\"after drop entity rowlength:\", len(df_full))\n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../data/underexpose_train/\"\n",
    "test_dir = \"../data/underexpose_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/underexpose_train/underexpose_train_click-1.csv\n",
      "../data/underexpose_train/underexpose_train_click-2.csv\n",
      "../data/underexpose_train/underexpose_train_click-0.csv\n",
      "before drop length: 727485\n",
      "after drop entity rowlength: 456951\n",
      "../data/underexpose_test/underexpose_test_click-1.csv\n",
      "../data/underexpose_test/underexpose_test_click-0.csv\n",
      "../data/underexpose_test/underexpose_test_click-2.csv\n",
      "before drop length: 68426\n",
      "after drop entity rowlength: 68418\n"
     ]
    }
   ],
   "source": [
    "train_data = gen_data(train_dir)\n",
    "test_data = gen_data(test_dir)\n",
    "whole_click = pd.concat([train_data, test_data])\n",
    "whole_click['rating'] = 1\n",
    "whole_qtime = test_data.drop_duplicates(subset=['user_id'])[['user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_stratified_split(whole_click, ratio=0.75, col_user='user_id', col_item='item_id', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:\n",
      "Total Ratings: 393650\n",
      "Unique Users: 23816\n",
      "Unique Items: 61791\n",
      "\n",
      "Test:\n",
      "Total Ratings: 131719\n",
      "Unique Users: 23692\n",
      "Unique Items: 50978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Train:\n",
    "Total Ratings: {train_total}\n",
    "Unique Users: {train_users}\n",
    "Unique Items: {train_items}\n",
    "\n",
    "Test:\n",
    "Total Ratings: {test_total}\n",
    "Unique Users: {test_users}\n",
    "Unique Items: {test_items}\n",
    "\"\"\".format(\n",
    "    train_total=len(train),\n",
    "    train_users=len(train['user_id'].unique()),\n",
    "    train_items=len(train['item_id'].unique()),\n",
    "    test_total=len(test),\n",
    "    test_users=len(test['user_id'].unique()),\n",
    "    test_items=len(test['item_id'].unique()),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, \n",
    "                    format='%(asctime)s %(levelname)-8s %(message)s')\n",
    "\n",
    "model = SAR(\n",
    "    col_user=\"user_id\",\n",
    "    col_item=\"item_id\",\n",
    "    col_rating=\"qtime\",\n",
    "    col_timestamp=\"timestamp\",\n",
    "    similarity_type=\"cooccurrence\", \n",
    "    time_decay_coefficient=3, \n",
    "    timedecay_formula=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-18 21:08:09,820 INFO     Collecting user affinity matrix\n",
      "2020-04-18 21:08:09,828 INFO     Calculating time-decayed affinities\n",
      "2020-04-18 21:08:09,974 INFO     Creating index columns\n",
      "2020-04-18 21:08:10,661 INFO     Building user affinity sparse matrix\n",
      "2020-04-18 21:08:10,681 INFO     Calculating item co-occurrence\n",
      "2020-04-18 21:08:11,751 INFO     Calculating item similarity\n",
      "2020-04-18 21:08:11,752 INFO     Using co-occurrence based similarity\n",
      "2020-04-18 21:08:11,753 INFO     Done training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.9589013820514083 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model.fit(whole_click)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-18 21:08:14,456 INFO     Calculating recommendation scores\n",
      "2020-04-18 21:08:25,982 INFO     Removing seen items\n"
     ]
    }
   ],
   "source": [
    "with Timer() as test_time:\n",
    "    top_k = model.recommend_k_items(test, remove_seen=True, top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 50\n",
    "args = [test, top_k]\n",
    "kwargs = dict(col_user='user_id', \n",
    "              col_item='item_id', \n",
    "              col_rating='qtime', \n",
    "              col_prediction='prediction', \n",
    "              relevancy_method='top_k', \n",
    "              k=TOP_K)\n",
    "\n",
    "\n",
    "eval_map = map_at_k(*args, **kwargs)\n",
    "eval_ndcg = ndcg_at_k(*args, **kwargs)\n",
    "eval_precision = precision_at_k(*args, **kwargs)\n",
    "eval_recall = recall_at_k(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "Top K:\t\t 50\n",
      "MAP:\t\t 0.000000\n",
      "NDCG:\t\t 0.000000\n",
      "Precision@K:\t 0.000000\n",
      "Recall@K:\t 0.000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model:\",\n",
    "      f\"Top K:\\t\\t {TOP_K}\",\n",
    "      f\"MAP:\\t\\t {eval_map:f}\",\n",
    "      f\"NDCG:\\t\\t {eval_ndcg:f}\",\n",
    "      f\"Precision@K:\\t {eval_precision:f}\",\n",
    "      f\"Recall@K:\\t {eval_recall:f}\", sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer() as test_time:\n",
    "    top_k = model.recommend_k_items(whole_qtime, remove_seen=True, top_k=50)\n",
    "\n",
    "print(\"Took {} seconds for prediction.\".format(test_time.interval))\n",
    "sub = top_k.groupby('user_id')['item_id'].apply(lambda x: ','.join(\n",
    "        [str(i) for i in x])).str.split(',', expand=True).reset_index()\n",
    "\n",
    "sub.to_csv(\"sub_sar_drop_dup.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
